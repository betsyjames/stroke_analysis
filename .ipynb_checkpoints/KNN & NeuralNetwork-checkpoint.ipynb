{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tabby_client'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-134dcef31992>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtabby_client\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tabby_client'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import seaborn as sns\n",
    "import tabby_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stroke_df = pd.read_csv(os.path.join('Resources/healthcare-dataset-stroke-data.csv'))\n",
    "stroke_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ID column is not necessary. Drop\n",
    "stroke_df = stroke_df.drop(['id'], axis = 1)\n",
    "stroke_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "201 rows of NA for bmi column only. Consider Drop NA but do complete data exploration first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking for nulls\n",
    "print(stroke_df.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking number of entries, data types\n",
    "stroke_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Value counts for hypertension column to ensure only two choices\n",
    "stroke_df[\"hypertension\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Value counts for heart diseease column to ensure only two choices\n",
    "stroke_df[\"heart_disease\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Value counts for stroke column to ensure only two choices\n",
    "stroke_df[\"stroke\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Only 249 patients with stroke. How many will we lose if we Drop NA for the bmi column?\n",
    "bmi_null_stroke = stroke_df.loc[(stroke_df['bmi'].isna() == True) & (stroke_df['stroke'] == 1)]\n",
    "len(bmi_null_stroke)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the value counts of all columns with datatype 'object' for a guide to encoding needs\n",
    "counts = stroke_df.select_dtypes(include=object).columns.tolist()\n",
    "(pd.DataFrame(\n",
    "    stroke_df[counts]\n",
    "    .melt(var_name='column', value_name='value')\n",
    "    .value_counts())\n",
    ".rename(columns={0: 'counts'})\n",
    ".sort_values(by=['column', 'counts']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Examining age column\n",
    "stroke_df.sort_values(by=['age']).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropped the \"Other\" in the \"Gender\" column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Children stroke count\n",
    "children_stroke = stroke_df.loc[(stroke_df['age'] < 17) & (stroke_df['stroke'] == 1)]\n",
    "len(children_stroke)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing children from the dataset\n",
    "\n",
    "no_children_df = stroke_df[stroke_df.age > 16]\n",
    "no_children_df.sort_values(by=['age'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ensure children is no longer listed as a work type\n",
    "no_children_df.sort_values(by=['age'])\n",
    "no_children_df['work_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop the 'other' row in gender column\n",
    "cleaned_stroke_df = no_children_df[no_children_df.gender != 'Other']\n",
    "cleaned_stroke_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Pandas get_dummies to convert categorical data\n",
    "\n",
    "encoded_df = pd.get_dummies(cleaned_stroke_df, columns=['gender','ever_married', 'Residence_type'])\n",
    "encoded_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop the extra dummies columns\n",
    "encoded_stroke_df = encoded_df.drop(columns=['gender_Female','ever_married_No','Residence_type_Rural'])\n",
    "encoded_stroke_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import label encoder \n",
    "from sklearn import preprocessing\n",
    "# label_encoder object knows how to understand word labels. \n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "# Encode labels in column 'Country'. \n",
    "encoded_stroke_df['work_type']= label_encoder.fit_transform(encoded_stroke_df['work_type']) \n",
    "encoded_stroke_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import label encoder \n",
    "from sklearn import preprocessing\n",
    "# label_encoder object knows how to understand word labels. \n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "# Encode labels in column 'Country'. \n",
    "encoded_stroke_df['smoking_status']= label_encoder.fit_transform(encoded_stroke_df['smoking_status']) \n",
    "encoded_stroke_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stroke_df = stroke_df[stroke_df['gender'] != 'Other']\n",
    "stroke_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Pandas get_dummies to convert categorical data for marital status and residence only\n",
    "\n",
    "stroke_df = pd.get_dummies(stroke_df, columns=['gender', 'ever_married', 'work_type', 'Residence_type', 'smoking_status'])\n",
    "stroke_df\n",
    "\n",
    "# Drop the extra dummies columns\n",
    "stroke_df = stroke_df.drop(columns=['gender_Female', 'ever_married_No','work_type_Never_worked', 'Residence_type_Rural', 'smoking_status_Unknown'])\n",
    "stroke_df\n",
    "# # Change column names back to the original\n",
    "stroke_df = stroke_df.rename(columns={\"ever_married_Yes\":\"ever_married\", \"work_type_Govt_job\": \"Govt_job\", \"work_type_Private\": \"Private\", \n",
    "    \"work_type_Self-employed\": \"Self-employed\", \"work_type_children\": \"children\", \"Residence_type_Urban\":\"Residence_type\", \"gender_Male\":\"gender\",\n",
    "    \"smoking_status_formerly smoked\":\"formerly smoked\", \"smoking_status_never smoked\": \"never smoked\", \"smoking_status_smokes\": \"smokes\"})\n",
    "stroke_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "40 is too many rows of valuable data to lose here. Consider other options. Insert mean into BMI? Other ideas?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bmi_mean = stroke_df[\"bmi\"].mean()\n",
    "stroke_df = stroke_df.fillna(bmi_mean)\n",
    "stroke_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop the age 17 and below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_age_df = stroke_df[stroke_df.age >= 17]\n",
    "drop_age_df.head()\n",
    "len(drop_age_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Value counts for hypertension column to ensure only binary encoding\n",
    "stroke_df[\"hypertension\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Value counts for heart diseease column to ensure only binary encoding\n",
    "stroke_df[\"heart_disease\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Value counts for stroke column to ensure only binary encoding\n",
    "stroke_df[\"stroke\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Only 249 patients with stroke. How many will we lose if we Drop NA for the bmi column?\n",
    "bmi_null_stroke = stroke_df.loc[(stroke_df['bmi'].isna() == True) & (stroke_df['stroke'] == 1)]\n",
    "len(bmi_null_stroke)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stroke_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(stroke_df.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = stroke_df[\"stroke\"]\n",
    "# target_names = [\"negative\", \"positive\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = stroke_df.drop(\"stroke\", axis=1)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from matplotlib import style\n",
    "style.use(\"ggplot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.datasets.samples_generator import make_blobs\n",
    "# X, y = make_blobs(n_samples=100, centers=2, random_state=0, cluster_std=.95)\n",
    "plt.scatter(X.iloc[:, 0], X.iloc[:, 1], c=y, s=100, cmap=\"bwr\");\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.iloc[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create a StandardScater model and fit it to the training data\n",
    "\n",
    "X_scaler = StandardScaler().fit(X_train)\n",
    "\n",
    "# Transform the training and testing data using the X_scaler and y_scaler models\n",
    "\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_model = KNeighborsClassifier()\n",
    "knn_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the KNN model to find the Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "knn_train_scores = []\n",
    "knn_test_scores = []\n",
    "for k in range(1, 20, 2):\n",
    "    knn_model = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn_model.fit(X_train_scaled, y_train)\n",
    "    knn_train_score = knn_model.score(X_train_scaled, y_train)\n",
    "    knn_test_score = knn_model.score(X_test_scaled, y_test)\n",
    "    knn_train_scores.append(knn_train_score)\n",
    "    knn_test_scores.append(knn_test_score)\n",
    "    print(f\"k: {k}, Train/Test Score: {knn_train_score:.3f}/{knn_test_score:.3f}\")\n",
    "    \n",
    "plt.plot(range(1, 20, 2), knn_train_scores, marker='o')\n",
    "plt.plot(range(1, 20, 2), knn_test_scores, marker=\"x\")\n",
    "plt.xlabel(\"k neighbors\")\n",
    "plt.ylabel(\"Testing accuracy Score\")\n",
    "plt.title(\"KNN Neighbors\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "#Create an informative confusion matrix using seaborn#Define variables for Information inside each box\n",
    "category = ['True Neg', 'False Pos', 'False Neg', 'True Pos']\n",
    "counts = ['{0:0.0f}'.format(value) for value in confusion_matrix.flatten()]\n",
    "percentages = ['{0:.1%}'.format(value) for value in confusion_matrix.flatten()/np.sum(confusion_matrix)]\n",
    "labels = [f'{v1}\\n{v2}\\n{v3}' for v1, v2, v3 in zip(category, counts, percentages)]#Convert all of the labels to an array and reshape\n",
    "labels = np.asarray(labels).reshape(2,2)#Change the 0,1 to no stroke and stroke for labels\n",
    "tick_labels = ['No Stroke', 'Stroke']#Calculate all of the values to be displayed underneath the matrix as xlabel. Calculate precision, recall, F1 for both\n",
    "#stroke and no stroke\n",
    "accuracy  = np.trace(confusion_matrix) / float(np.sum(confusion_matrix))\n",
    "precision_stroke = confusion_matrix[1,1] / sum(confusion_matrix[:,1])\n",
    "recall_stroke    = confusion_matrix[1,1] / sum(confusion_matrix[1,:])\n",
    "f1_score_stroke  = 2*precision_stroke*recall_stroke / (precision_stroke + recall_stroke)\n",
    "precision = confusion_matrix[0,0] / sum(confusion_matrix[:,0])\n",
    "recall    = confusion_matrix[0,0] / sum(confusion_matrix[0,:])\n",
    "f1_score  = 2*precision*recall / (precision + recall)#Set up the text for the x label display underneath the matrix\n",
    "stats_text = \"\\n\\nAccuracy={:0.2f}\\nPrecision (stroke)={:0.2f}, Precision (no stroke)={:0.2f}\\nRecall (stroke)={:0.2f}, Recall (no stroke)={:0.2f}\\nF1 Score (stroke)={:0.2f}, F1 Score (no stroke)={:0.2f} \".format(accuracy,precision_stroke, precision, recall_stroke, recall, f1_score_stroke, f1_score)#Make the seaborn heatmap\n",
    "sns.heatmap(confusion_matrix, annot = labels, fmt = '', xticklabels = tick_labels, yticklabels = tick_labels)\n",
    "plt.xlabel(stats_text, fontsize = 14)\n",
    "plt.title(\"KNN Neigl\", fontsize = 14)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the GridSearch estimator along with a parameter object containing the values to adjust\n",
    "knn_param_grid = {'n_neighbors': range(1, 20, 2)}\n",
    "knn_grid = GridSearchCV(knn_model, knn_param_grid, verbose=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_grid.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions with the hypertuned model\n",
    "knn_predictions = knn_grid.predict(X_test_scaled)\n",
    "knn_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test Acc: %.3f' % knn_grid.score(X_test_scaled, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate classification report\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, knn_predictions,\n",
    "                            target_names=[\"negative\", \"positive\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that k: 13 seems to be the best choice for this dataset\n",
    "knn = KNeighborsClassifier(n_neighbors=7)\n",
    "knn.fit(X_train_scaled, y_train)\n",
    "print('k=7 Test Acc: %.3f' % knn.score(X_test_scaled, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Networks with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the seed value for the notebook so the results are reproducible\n",
    "from numpy.random import seed\n",
    "seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding\n",
    "y_train_categorical = to_categorical(y_train)\n",
    "y_test_categorical = to_categorical(y_test)\n",
    "y_train_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining our Model Architecture (the layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense\n",
    "number_inputs = 15\n",
    "number_hidden_nodes = 4\n",
    "model.add(Dense(units=number_hidden_nodes,\n",
    "                activation='relu', input_dim=number_inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_classes = 2\n",
    "model.add(Dense(units=number_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit (train) the model\n",
    "model.fit(\n",
    "    X_train_scaled,\n",
    "    y_train_categorical,\n",
    "    epochs=1000,\n",
    "    shuffle=True,\n",
    "    verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantifying the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model using the testing data\n",
    "model_loss, model_accuracy = model.evaluate(\n",
    "    X_test_scaled, y_test_categorical, verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
